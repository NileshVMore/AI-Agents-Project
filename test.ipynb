{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0022939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c858f105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc381c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed782537",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_llm= ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb922d14",
   "metadata": {},
   "outputs": [
    {
     "ename": "ChatGoogleGenerativeAIError",
     "evalue": "Error calling model 'gemini-2.0-flash' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\nPlease retry in 21.119124831s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.0-flash', 'location': 'global'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '21s'}]}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AI-Agents-Project\\env\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:3047\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   3046\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3047\u001b[39m     response: GenerateContentResponse = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3048\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3049\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3050\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AI-Agents-Project\\env\\Lib\\site-packages\\google\\genai\\models.py:5474\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5473\u001b[39m i += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m5474\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5475\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparsed_config\u001b[49m\n\u001b[32m   5476\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5478\u001b[39m function_map = _extra_utils.get_function_map(parsed_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AI-Agents-Project\\env\\Lib\\site-packages\\google\\genai\\models.py:4214\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   4212\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m4214\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4215\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   4216\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4218\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   4219\u001b[39m     config, \u001b[33m'\u001b[39m\u001b[33mshould_return_http_response\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4220\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AI-Agents-Project\\env\\Lib\\site-packages\\google\\genai\\_api_client.py:1386\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m   1383\u001b[39m http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m   1384\u001b[39m     http_method, path, request_dict, http_options\n\u001b[32m   1385\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1386\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1387\u001b[39m response_body = (\n\u001b[32m   1388\u001b[39m     response.response_stream[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response.response_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1389\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AI-Agents-Project\\env\\Lib\\site-packages\\google\\genai\\_api_client.py:1220\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, http_options, stream)\u001b[39m\n\u001b[32m   1219\u001b[39m     retry = tenacity.Retrying(**retry_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1220\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[32m   1222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retry(\u001b[38;5;28mself\u001b[39m._request_once, http_request, stream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AI-Agents-Project\\env\\Lib\\site-packages\\tenacity\\__init__.py:470\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m470\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    471\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AI-Agents-Project\\env\\Lib\\site-packages\\tenacity\\__init__.py:371\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AI-Agents-Project\\env\\Lib\\site-packages\\tenacity\\__init__.py:413\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AI-Agents-Project\\env\\Lib\\site-packages\\tenacity\\__init__.py:184\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m     \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AI-Agents-Project\\env\\Lib\\site-packages\\tenacity\\__init__.py:473\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AI-Agents-Project\\env\\Lib\\site-packages\\google\\genai\\_api_client.py:1199\u001b[39m, in \u001b[36mBaseApiClient._request_once\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m   1192\u001b[39m response = \u001b[38;5;28mself\u001b[39m._httpx_client.request(\n\u001b[32m   1193\u001b[39m     method=http_request.method,\n\u001b[32m   1194\u001b[39m     url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1197\u001b[39m     timeout=http_request.timeout,\n\u001b[32m   1198\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1199\u001b[39m \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m   1201\u001b[39m     response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m   1202\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AI-Agents-Project\\env\\Lib\\site-packages\\google\\genai\\errors.py:134\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m    132\u001b[39m   response_json = response.body_segments[\u001b[32m0\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m, {})\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AI-Agents-Project\\env\\Lib\\site-packages\\google\\genai\\errors.py:159\u001b[39m, in \u001b[36mAPIError.raise_error\u001b[39m\u001b[34m(cls, status_code, response_json, response)\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n",
      "\u001b[31mClientError\u001b[39m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\nPlease retry in 21.119124831s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.0-flash', 'location': 'global'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '21s'}]}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mchat_llm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHello, How are you?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AI-Agents-Project\\env\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:2535\u001b[39m, in \u001b[36mChatGoogleGenerativeAI.invoke\u001b[39m\u001b[34m(self, input, config, code_execution, stop, **kwargs)\u001b[39m\n\u001b[32m   2532\u001b[39m         msg = \u001b[33m\"\u001b[39m\u001b[33mTools are already defined.code_execution tool can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt be defined\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2533\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m-> \u001b[39m\u001b[32m2535\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AI-Agents-Project\\env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:402\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    390\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m     **kwargs: Any,\n\u001b[32m    396\u001b[39m ) -> AIMessage:\n\u001b[32m    397\u001b[39m     config = ensure_config(config)\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    400\u001b[39m         cast(\n\u001b[32m    401\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    412\u001b[39m         ).message,\n\u001b[32m    413\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AI-Agents-Project\\env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1121\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1112\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1114\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1118\u001b[39m     **kwargs: Any,\n\u001b[32m   1119\u001b[39m ) -> LLMResult:\n\u001b[32m   1120\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AI-Agents-Project\\env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:931\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    929\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    930\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    937\u001b[39m         )\n\u001b[32m    938\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    939\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AI-Agents-Project\\env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1233\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1231\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1232\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1233\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1237\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AI-Agents-Project\\env\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:3051\u001b[39m, in \u001b[36mChatGoogleGenerativeAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)\u001b[39m\n\u001b[32m   3047\u001b[39m     response: GenerateContentResponse = \u001b[38;5;28mself\u001b[39m.client.models.generate_content(\n\u001b[32m   3048\u001b[39m         **request,\n\u001b[32m   3049\u001b[39m     )\n\u001b[32m   3050\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m3051\u001b[39m     \u001b[43m_handle_client_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3053\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _response_to_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AI-Agents-Project\\env\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:145\u001b[39m, in \u001b[36m_handle_client_error\u001b[39m\u001b[34m(e, request)\u001b[39m\n\u001b[32m    143\u001b[39m model_name = request.get(\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33munknown\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    144\u001b[39m msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError calling model \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me.status\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m ChatGoogleGenerativeAIError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mChatGoogleGenerativeAIError\u001b[39m: Error calling model 'gemini-2.0-flash' (RESOURCE_EXHAUSTED): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\\nPlease retry in 21.119124831s.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_input_token_count', 'quotaId': 'GenerateContentInputTokensPerModelPerMinute-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash'}}, {'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerDayPerProjectPerModel-FreeTier', 'quotaDimensions': {'model': 'gemini-2.0-flash', 'location': 'global'}}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '21s'}]}}"
     ]
    }
   ],
   "source": [
    "chat_llm.invoke(\"Hello, How are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83b21089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from  typing_extensions import Annotated \n",
    "import operator\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb093c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage, SystemMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc21d9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GraphState(TypedDict):\n",
    "    message: Annotated[list[AnyMessage] , operator.add ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30e0341a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call(state: GraphState) -> dict:\n",
    "    \"\"\"Call the LLM using conversation messages and append AI response.\"\"\"\n",
    "    response = chat_llm.invoke(state[\"messages\"])  # AIMessage\n",
    "    return {\n",
    "        \"messages\": [response]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b53e3716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_counter(state: GraphState) -> dict:\n",
    "    \"\"\"Count tokens (simple word count) in the last AI message.\"\"\"\n",
    "    last_msg = state[\"messages\"][-1]\n",
    "    text = last_msg.content\n",
    "    token_number = len(text.split())\n",
    "    summary = f\"Total token number in the generated answer (word count) is {token_number}\"\n",
    "    return {\n",
    "        \"messages\": [AIMessage(content=summary)]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6123ea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01f8c990",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(GraphState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6a0c958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x283b5ca72f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.add_node(\"llm_call\", llm_call)\n",
    "builder.add_node(\"token_counter\", token_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "184d4d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x283b5ca72f0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder.set_entry_point(\"llm_call\")\n",
    "builder.add_edge(\"llm_call\", \"token_counter\")\n",
    "builder.set_finish_point(\"token_counter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7c8133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4eb62c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(nodes={'__start__': Node(id='__start__', name='__start__', data=RunnableCallable(tags=None, recurse=True, explode_args=False, func_accepts={}), metadata=None), 'llm_call': Node(id='llm_call', name='llm_call', data=llm_call(tags=None, recurse=True, explode_args=False, func_accepts={}), metadata=None), 'token_counter': Node(id='token_counter', name='token_counter', data=token_counter(tags=None, recurse=True, explode_args=False, func_accepts={}), metadata=None), '__end__': Node(id='__end__', name='__end__', data=None, metadata=None)}, edges=[Edge(source='__start__', target='llm_call', data=None, conditional=False), Edge(source='llm_call', target='token_counter', data=None, conditional=False), Edge(source='token_counter', target='__end__', data=None, conditional=False)])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.get_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab25eb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00dfa698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "737ea333",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chat_llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m result = \u001b[43mapp\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHi, this is Nilesh. Say hello in detail.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AI-Agents-Project\\env\\Lib\\site-packages\\langgraph\\pregel\\main.py:3071\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3068\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3069\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3071\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3084\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3085\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3086\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AI-Agents-Project\\env\\Lib\\site-packages\\langgraph\\pregel\\main.py:2646\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2644\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2645\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2646\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2653\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2655\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2656\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AI-Agents-Project\\env\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AI-Agents-Project\\env\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AI-Agents-Project\\env\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AI-Agents-Project\\env\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mllm_call\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mllm_call\u001b[39m(state: GraphState) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the LLM using conversation messages and append AI response.\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     response = \u001b[43mchat_llm\u001b[49m.invoke(state[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m])  \u001b[38;5;66;03m# AIMessage\u001b[39;00m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m      5\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [response]\n\u001b[32m      6\u001b[39m     }\n",
      "\u001b[31mNameError\u001b[39m: name 'chat_llm' is not defined",
      "During task with name 'llm_call' and id '39a1ffb2-aa0b-0d6d-2252-ae7597e21aa9'"
     ]
    }
   ],
   "source": [
    "result = app.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"Hi, this is Nilesh. Say hello in detail.\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd2f4bd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresult\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m      2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(m).\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m, m.content)\n",
      "\u001b[31mNameError\u001b[39m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "for m in result[\"messages\"]:\n",
    "    print(type(m).__name__, \":\", m.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dd06cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
